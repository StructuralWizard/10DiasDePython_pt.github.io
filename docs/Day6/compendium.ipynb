{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comprehensive Data Analysis Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Comprehensive Data Analysis Notebook\n",
        "\n",
        "This notebook provides a consolidated example of the key features found across multiple data science projects. It covers the end-to-end workflow from loading data to building and evaluating a machine learning model.\n",
        "\n",
        "**Features Covered:**\n",
        "1.  **Data Loading and Exploration**: Reading data, basic inspection.\n",
        "2.  **Data Manipulation**: Cleaning, transforming, and aggregating data.\n",
        "3.  **Data Visualization**: Using Matplotlib, Seaborn, and Plotly for insights.\n",
        "4.  **Machine Learning and Statistics**: Performing statistical tests and building a regression model."
      ],
      "metadata": {
        "id": "intro_section"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup: Importing Necessary Libraries\n",
        "\n",
        "First, we import all the Python libraries we'll need for our analysis. This includes `pandas` for data manipulation, `numpy` for numerical operations, `matplotlib`, `seaborn`, and `plotly` for visualizations, and `scikit-learn` and `scipy` for machine learning and statistics."
      ],
      "metadata": {
        "id": "imports_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Machine Learning and Statistics\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "from scipy import stats\n",
        "\n",
        "# To handle string data as a file for reading CSV\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "imports_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "This section covers loading data into our environment. We'll demonstrate reading from a CSV file and then explore the basic properties of our dataset to get a first understanding of its structure and content."
      ],
      "metadata": {
        "id": "data_loading_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading from a CSV\n",
        "\n",
        "Here, we simulate a CSV file using a string and then read it into a pandas DataFrame. This is a common first step when your data is stored in a `.csv` file."
      ],
      "metadata": {
        "id": "read_csv_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = '''product_id,product_name,price,launch_date\n",
        "101,Gadget A,199.99,2023-01-15\n",
        "102,Widget B,49.50,2023-02-20\n",
        "103,Thing C,89.00,\n",
        "104,Device D,249.99,2023-04-10\n",
        "104,Device D,249.99,2023-04-10\n",
        "105,Gizmo E,120.00,2023-05-25'''\n",
        "\n",
        "data_file = StringIO(csv_data)\n",
        "df_products = pd.read_csv(data_file)\n",
        "\n",
        "print(\"Product data loaded successfully!\")"
      ],
      "metadata": {
        "id": "read_csv_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial DataFrame Exploration\n",
        "\n",
        "We use methods like `.head()`, `.shape`, and `.info()` to inspect the first few rows, check the dimensions (rows, columns), and get a summary of data types and non-null values."
      ],
      "metadata": {
        "id": "explore_df_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First 5 rows of the product data:\")\n",
        "display(df_products.head())\n",
        "\n",
        "print(f\"\\nDataset dimensions (rows, columns): {df_products.shape}\")\n",
        "\n",
        "print(\"\\nData types and non-null values:\")\n",
        "df_products.info()"
      ],
      "metadata": {
        "id": "explore_df_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Manipulation\n",
        "\n",
        "Data is rarely perfect. This section covers essential data manipulation techniques like cleaning (handling duplicates and missing values), type conversion, and aggregation to prepare the data for analysis and modeling."
      ],
      "metadata": {
        "id": "data_manipulation_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning: Duplicates and Missing Values\n",
        "\n",
        "We first check for and remove duplicate rows. Then, we identify columns with missing data and fill them using an appropriate strategy (in this case, filling the missing date with the most frequent date)."
      ],
      "metadata": {
        "id": "cleaning_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Handling Duplicates ---\n",
        "print(f\"Number of duplicate rows: {df_products.duplicated().sum()}\")\n",
        "df_products.drop_duplicates(inplace=True)\n",
        "print(f\"Number of duplicates after cleaning: {df_products.duplicated().sum()}\")\n",
        "\n",
        "# --- Handling Missing Data ---\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_products.isna().sum())\n",
        "\n",
        "# Fill missing launch_date with the mode (most frequent value)\n",
        "mode_date = df_products['launch_date'].mode()[0]\n",
        "df_products['launch_date'].fillna(mode_date, inplace=True)\n",
        "\n",
        "print(\"\\nMissing values after filling:\")\n",
        "print(df_products.isna().sum())"
      ],
      "metadata": {
        "id": "cleaning_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Type Conversion\n",
        "\n",
        "Columns are often read with incorrect data types (e.g., dates as strings). Here, we convert the `launch_date` column from a generic object to a proper `datetime` type, which enables time-based analysis."
      ],
      "metadata": {
        "id": "type_conversion_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data types before conversion:\")\n",
        "print(df_products.dtypes)\n",
        "\n",
        "df_products['launch_date'] = pd.to_datetime(df_products['launch_date'])\n",
        "\n",
        "print(\"\\nData types after conversion:\")\n",
        "print(df_products.dtypes)"
      ],
      "metadata": {
        "id": "type_conversion_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Aggregation\n",
        "\n",
        "To summarize data, we use `groupby()`. Here, we create a new 'launch_month' column and then group by it to calculate the average price of products launched each month."
      ],
      "metadata": {
        "id": "aggregation_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_products['launch_month'] = df_products['launch_date'].dt.month_name()\n",
        "avg_price_by_month = df_products.groupby('launch_month')['price'].mean().reset_index()\n",
        "\n",
        "print(\"Average product price by launch month:\")\n",
        "display(avg_price_by_month)"
      ],
      "metadata": {
        "id": "aggregation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy Array Manipulation\n",
        "\n",
        "`NumPy` is the foundation of numerical computing in Python. Here, we create a 2D array, inspect its properties, and perform basic operations like slicing to select a subset of the data and applying a mathematical function."
      ],
      "metadata": {
        "id": "numpy_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 3x4 NumPy array of random numbers\n",
        "my_array = np.random.rand(3, 4) * 100\n",
        "\n",
        "print(\"Original NumPy Array:\")\n",
        "print(my_array)\n",
        "\n",
        "print(f\"\\nShape: {my_array.shape}\")\n",
        "print(f\"Data Type: {my_array.dtype}\")\n",
        "\n",
        "# Slicing: get the first 2 rows and last 2 columns\n",
        "subset = my_array[:2, 2:]\n",
        "print(\"\\nSliced Subset:\")\n",
        "print(subset)\n",
        "\n",
        "# Applying a function: find the square root of all elements\n",
        "sqrt_array = np.sqrt(my_array)\n",
        "print(\"\\nArray after applying sqrt:\")\n",
        "print(sqrt_array.round(2))"
      ],
      "metadata": {
        "id": "numpy_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Visualization\n",
        "\n",
        "Visualization is key to uncovering patterns and communicating findings. This section demonstrates how to create plots using `matplotlib`, `seaborn`, and `plotly`."
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization with Matplotlib\n",
        "\n",
        "`Matplotlib` is a versatile library for creating static plots. Here, we create a bar chart to visualize the average product prices by launch month that we calculated earlier."
      ],
      "metadata": {
        "id": "matplotlib_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(avg_price_by_month['launch_month'], avg_price_by_month['price'], color='skyblue')\n",
        "plt.title('Average Product Price by Launch Month', fontsize=16)\n",
        "plt.xlabel('Month', fontsize=12)\n",
        "plt.ylabel('Average Price ($)', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "matplotlib_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization with Seaborn\n",
        "\n",
        "`Seaborn` is built on top of Matplotlib and is excellent for statistical visualizations. We will load the California Housing dataset to demonstrate a regression plot, which shows the relationship between two variables along with a linear regression line."
      ],
      "metadata": {
        "id": "seaborn_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a sample dataset for regression analysis\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "df_housing = housing.frame\n",
        "\n",
        "# Seaborn Regression Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(data=df_housing, x='MedInc', y='MedHouseVal', \n",
        "            scatter_kws={'alpha':0.3}, line_kws={'color':'red'})\n",
        "plt.title('Median Income vs. Median House Value in California', fontsize=16)\n",
        "plt.xlabel('Median Income (in tens of thousands of $)', fontsize=12)\n",
        "plt.ylabel('Median House Value (in hundreds of thousands of $)', fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "seaborn_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization with Plotly\n",
        "\n",
        "`Plotly` creates interactive visualizations that are great for web-based dashboards and exploration. Below is an interactive scatter plot where you can hover over points to see details."
      ],
      "metadata": {
        "id": "plotly_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using a sample of the data to keep the plot from being too crowded\n",
        "df_sample = df_housing.sample(n=1000, random_state=42)\n",
        "\n",
        "fig = px.scatter(df_sample, \n",
        "                 x='Longitude', \n",
        "                 y='Latitude', \n",
        "                 color='MedHouseVal', \n",
        "                 size='Population',\n",
        "                 hover_name='MedHouseVal',\n",
        "                 color_continuous_scale=px.colors.sequential.Viridis,\n",
        "                 title='California Housing: Value by Geo-location')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "plotly_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Machine Learning and Statistics\n",
        "\n",
        "This final section applies statistical testing and machine learning. We'll perform a t-test to compare two groups and then build a multivariable linear regression model to predict housing values."
      ],
      "metadata": {
        "id": "ml_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical Analysis: Independent T-test\n",
        "\n",
        "A t-test is used to determine if there is a significant difference between the means of two groups. Here, we create two sample groups and use `scipy.stats.ttest_ind` to see if their means are statistically different."
      ],
      "metadata": {
        "id": "stats_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two independent samples\n",
        "group_a = np.random.normal(loc=105, scale=10, size=50)\n",
        "group_b = np.random.normal(loc=100, scale=10, size=50)\n",
        "\n",
        "# Perform independent t-test\n",
        "t_stat, p_value = stats.ttest_ind(a=group_a, b=group_b)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"\\nThe difference between the groups is statistically significant (p < 0.05).\")\n",
        "else:\n",
        "    print(\"\\nThe difference between the groups is not statistically significant (p >= 0.05).\")"
      ],
      "metadata": {
        "id": "stats_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariable Linear Regression\n",
        "\n",
        "We'll build a model to predict the median house value (`MedHouseVal`) using multiple features like median income, average rooms, and population. This involves:\n",
        "1.  Defining features (X) and the target (y).\n",
        "2.  Splitting the data into training and testing sets.\n",
        "3.  Training the regression model.\n",
        "4.  Evaluating its performance."
      ],
      "metadata": {
        "id": "regression_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define features (X) and target (y)\n",
        "features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
        "X = df_housing[features]\n",
        "y = df_housing['MedHouseVal']\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Create and train the model\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate\n",
        "y_pred = regression_model.predict(X_test)\n",
        "r2_score = metrics.r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model R-squared score: {r2_score:.4f}\")\n",
        "\n",
        "# Displaying the model coefficients\n",
        "coefficients = pd.DataFrame(regression_model.coef_, X.columns, columns=['Coefficient'])\n",
        "print(\"\\nModel Coefficients:\")\n",
        "display(coefficients)"
      ],
      "metadata": {
        "id": "regression_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
